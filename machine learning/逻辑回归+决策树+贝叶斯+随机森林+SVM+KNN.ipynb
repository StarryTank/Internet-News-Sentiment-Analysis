{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#首先将用到的包进行导入\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import model_selection \n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>评论</th>\n",
       "      <th>评分</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>问责 领导 上 黄镇 党委书记 张涛 宣国 才 真能 一手遮天 几天 看 有人 举报 施 某...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>江歌 事件 教会 孩子 善良 更要 懂得 保护 ! 过去 一年 江歌 悲剧 几日 再次 刷屏...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>绝味 鸭 脖 广告 开 黄腔 引 众怒   双 11 拼值 双 11 1600 亿 销售额 ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>央视 曝光 ! 如东 医药企业 槽罐车 改成 垃圾车 夜间 偷排 高浓度 废水 2016 年...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>恶劣 至极 央视 都 曝光 ! 南通 如东 医药企业 槽罐车 改成 洒水车 夜间 偷排 高浓...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  评论  评分\n",
       "0  问责 领导 上 黄镇 党委书记 张涛 宣国 才 真能 一手遮天 几天 看 有人 举报 施 某...   2\n",
       "1  江歌 事件 教会 孩子 善良 更要 懂得 保护 ! 过去 一年 江歌 悲剧 几日 再次 刷屏...   1\n",
       "2  绝味 鸭 脖 广告 开 黄腔 引 众怒   双 11 拼值 双 11 1600 亿 销售额 ...   2\n",
       "3  央视 曝光 ! 如东 医药企业 槽罐车 改成 垃圾车 夜间 偷排 高浓度 废水 2016 年...   2\n",
       "4  恶劣 至极 央视 都 曝光 ! 南通 如东 医药企业 槽罐车 改成 洒水车 夜间 偷排 高浓...   2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#将数据进行读取\n",
    "data=pd.read_csv('./reviews_score_update.csv',index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5872,) (1468,)\n"
     ]
    }
   ],
   "source": [
    "#现在是划分数据集\n",
    "#random_state 取值，这是为了在不同环境中，保证随机数取值一致，以便验证模型的实际效果。\n",
    "train_x,test_x,train_y,test_y=model_selection.train_test_split(data.评论.values.astype('U'),data.评分.values,test_size=0.2,random_state=1)\n",
    " \n",
    "#划分完毕，查看数据形状\n",
    "print(train_x.shape,test_x.shape)\n",
    "#train_x 训练集数据 test_x 测试集数据  train_y训练集的标签 test_y 测试集的标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义函数，从哈工大中文停用词表里面，把停用词作为列表格式保存并返回 在这里加上停用词表是因为TfidfVectorizer和CountVectorizer的函数中\n",
    "#可以根据提供用词里列表进行去停用词\n",
    "def get_stopwords(stop_word_file):\n",
    "    with open(stop_word_file,encoding='UTF-8') as f:\n",
    "        stopwords=f.read()\n",
    "    stopwords_list=stopwords.split('\\n')\n",
    "    custom_stopwords_list=[i for i in stopwords_list]\n",
    "    return custom_stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获得由停用词组成的列表\n",
    "stop_words_file = './hit_stopwords.txt'\n",
    "stopwords = get_stopwords(stop_words_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['———',\n",
       " '》），',\n",
       " '）÷（１－',\n",
       " '”，',\n",
       " '）、',\n",
       " '＝（',\n",
       " ':',\n",
       " '→',\n",
       " '℃ ',\n",
       " '&',\n",
       " '*',\n",
       " '一一',\n",
       " '~~~~',\n",
       " '’',\n",
       " '. ',\n",
       " '『',\n",
       " '.一',\n",
       " './',\n",
       " '-- ',\n",
       " '』',\n",
       " '＝″',\n",
       " '【',\n",
       " '［＊］',\n",
       " '｝＞',\n",
       " '［⑤］］',\n",
       " '［①Ｄ］',\n",
       " 'ｃ］',\n",
       " 'ｎｇ昉',\n",
       " '＊',\n",
       " '//',\n",
       " '［',\n",
       " '］',\n",
       " '［②ｅ］',\n",
       " '［②ｇ］',\n",
       " '＝｛',\n",
       " '}',\n",
       " '，也 ',\n",
       " '‘',\n",
       " 'Ａ',\n",
       " '［①⑥］',\n",
       " '［②Ｂ］ ',\n",
       " '［①ａ］',\n",
       " '［④ａ］',\n",
       " '［①③］',\n",
       " '［③ｈ］',\n",
       " '③］',\n",
       " '１． ',\n",
       " '－－ ',\n",
       " '［②ｂ］',\n",
       " '’‘ ',\n",
       " '××× ',\n",
       " '［①⑧］',\n",
       " '０：２ ',\n",
       " '＝［',\n",
       " '［⑤ｂ］',\n",
       " '［②ｃ］ ',\n",
       " '［④ｂ］',\n",
       " '［②③］',\n",
       " '［③ａ］',\n",
       " '［④ｃ］',\n",
       " '［①⑤］',\n",
       " '［①⑦］',\n",
       " '［①ｇ］',\n",
       " '∈［ ',\n",
       " '［①⑨］',\n",
       " '［①④］',\n",
       " '［①ｃ］',\n",
       " '［②ｆ］',\n",
       " '［②⑧］',\n",
       " '［②①］',\n",
       " '［①Ｃ］',\n",
       " '［③ｃ］',\n",
       " '［③ｇ］',\n",
       " '［②⑤］',\n",
       " '［②②］',\n",
       " '一.',\n",
       " '［①ｈ］',\n",
       " '.数',\n",
       " '［］',\n",
       " '［①Ｂ］',\n",
       " '数/',\n",
       " '［①ｉ］',\n",
       " '［③ｅ］',\n",
       " '［①①］',\n",
       " '［④ｄ］',\n",
       " '［④ｅ］',\n",
       " '［③ｂ］',\n",
       " '［⑤ａ］',\n",
       " '［①Ａ］',\n",
       " '［②⑧］',\n",
       " '［②⑦］',\n",
       " '［①ｄ］',\n",
       " '［②ｊ］',\n",
       " '〕〔',\n",
       " '］［',\n",
       " '://',\n",
       " '′∈',\n",
       " '［②④',\n",
       " '［⑤ｅ］',\n",
       " '１２％',\n",
       " 'ｂ］',\n",
       " '...',\n",
       " '...................',\n",
       " '…………………………………………………③',\n",
       " 'ＺＸＦＩＴＬ',\n",
       " '［③Ｆ］',\n",
       " '」',\n",
       " '［①ｏ］',\n",
       " '］∧′＝［ ',\n",
       " '∪φ∈',\n",
       " '′｜',\n",
       " '｛－',\n",
       " '②ｃ',\n",
       " '｝',\n",
       " '［③①］',\n",
       " 'Ｒ．Ｌ．',\n",
       " '［①Ｅ］',\n",
       " 'Ψ',\n",
       " '－［＊］－',\n",
       " '↑',\n",
       " '.日 ',\n",
       " '［②ｄ］',\n",
       " '［②',\n",
       " '［②⑦］',\n",
       " '［②②］',\n",
       " '［③ｅ］',\n",
       " '［①ｉ］',\n",
       " '［①Ｂ］',\n",
       " '［①ｈ］',\n",
       " '［①ｄ］',\n",
       " '［①ｇ］',\n",
       " '［①②］',\n",
       " '［②ａ］',\n",
       " 'ｆ］',\n",
       " '［⑩］',\n",
       " 'ａ］',\n",
       " '［①ｅ］',\n",
       " '［②ｈ］',\n",
       " '［②⑥］',\n",
       " '［③ｄ］',\n",
       " '［②⑩］',\n",
       " 'ｅ］',\n",
       " '〉',\n",
       " '】',\n",
       " '元／吨',\n",
       " '［②⑩］',\n",
       " '２．３％',\n",
       " '５：０  ',\n",
       " '［①］',\n",
       " '::',\n",
       " '［②］',\n",
       " '［③］',\n",
       " '［④］',\n",
       " '［⑤］',\n",
       " '［⑥］',\n",
       " '［⑦］',\n",
       " '［⑧］',\n",
       " '［⑨］ ',\n",
       " '……',\n",
       " '——',\n",
       " '?',\n",
       " '、',\n",
       " '。',\n",
       " '“',\n",
       " '”',\n",
       " '《',\n",
       " '》',\n",
       " '！',\n",
       " '，',\n",
       " '：',\n",
       " '；',\n",
       " '？',\n",
       " '．',\n",
       " ',',\n",
       " '．',\n",
       " \"'\",\n",
       " '? ',\n",
       " '·',\n",
       " '———',\n",
       " '──',\n",
       " '? ',\n",
       " '—',\n",
       " '<',\n",
       " '>',\n",
       " '（',\n",
       " '）',\n",
       " '〔',\n",
       " '〕',\n",
       " '[',\n",
       " ']',\n",
       " '(',\n",
       " ')',\n",
       " '-',\n",
       " '+',\n",
       " '～',\n",
       " '×',\n",
       " '／',\n",
       " '/',\n",
       " '①',\n",
       " '②',\n",
       " '③',\n",
       " '④',\n",
       " '⑤',\n",
       " '⑥',\n",
       " '⑦',\n",
       " '⑧',\n",
       " '⑨',\n",
       " '⑩',\n",
       " 'Ⅲ',\n",
       " 'В',\n",
       " '\"',\n",
       " ';',\n",
       " '#',\n",
       " '@',\n",
       " 'γ',\n",
       " 'μ',\n",
       " 'φ',\n",
       " 'φ．',\n",
       " '× ',\n",
       " 'Δ',\n",
       " '■',\n",
       " '▲',\n",
       " 'sub',\n",
       " 'exp ',\n",
       " 'sup',\n",
       " 'sub',\n",
       " 'Lex ',\n",
       " '＃',\n",
       " '％',\n",
       " '＆',\n",
       " '＇',\n",
       " '＋',\n",
       " '＋ξ',\n",
       " '＋＋',\n",
       " '－',\n",
       " '－β',\n",
       " '＜',\n",
       " '＜±',\n",
       " '＜Δ',\n",
       " '＜λ',\n",
       " '＜φ',\n",
       " '＜＜',\n",
       " '=',\n",
       " '＝',\n",
       " '＝☆',\n",
       " '＝－',\n",
       " '＞',\n",
       " '＞λ',\n",
       " '＿',\n",
       " '～±',\n",
       " '～＋',\n",
       " '［⑤ｆ］',\n",
       " '［⑤ｄ］',\n",
       " '［②ｉ］',\n",
       " '≈ ',\n",
       " '［②Ｇ］',\n",
       " '［①ｆ］',\n",
       " 'ＬＩ',\n",
       " '㈧ ',\n",
       " '［－',\n",
       " '......',\n",
       " '〉',\n",
       " '［③⑩］',\n",
       " '第二',\n",
       " '一番',\n",
       " '一直',\n",
       " '一个',\n",
       " '一些',\n",
       " '许多',\n",
       " '种',\n",
       " '有的是',\n",
       " '也就是说',\n",
       " '末##末',\n",
       " '啊',\n",
       " '阿',\n",
       " '哎',\n",
       " '哎呀',\n",
       " '哎哟',\n",
       " '唉',\n",
       " '俺',\n",
       " '俺们',\n",
       " '按',\n",
       " '按照',\n",
       " '吧',\n",
       " '吧哒',\n",
       " '把',\n",
       " '罢了',\n",
       " '被',\n",
       " '本',\n",
       " '本着',\n",
       " '比',\n",
       " '比方',\n",
       " '比如',\n",
       " '鄙人',\n",
       " '彼',\n",
       " '彼此',\n",
       " '边',\n",
       " '别',\n",
       " '别的',\n",
       " '别说',\n",
       " '并',\n",
       " '并且',\n",
       " '不比',\n",
       " '不成',\n",
       " '不单',\n",
       " '不但',\n",
       " '不独',\n",
       " '不管',\n",
       " '不光',\n",
       " '不过',\n",
       " '不仅',\n",
       " '不拘',\n",
       " '不论',\n",
       " '不怕',\n",
       " '不然',\n",
       " '不如',\n",
       " '不特',\n",
       " '不惟',\n",
       " '不问',\n",
       " '不只',\n",
       " '朝',\n",
       " '朝着',\n",
       " '趁',\n",
       " '趁着',\n",
       " '乘',\n",
       " '冲',\n",
       " '除',\n",
       " '除此之外',\n",
       " '除非',\n",
       " '除了',\n",
       " '此',\n",
       " '此间',\n",
       " '此外',\n",
       " '从',\n",
       " '从而',\n",
       " '打',\n",
       " '待',\n",
       " '但',\n",
       " '但是',\n",
       " '当',\n",
       " '当着',\n",
       " '到',\n",
       " '得',\n",
       " '的',\n",
       " '的话',\n",
       " '等',\n",
       " '等等',\n",
       " '地',\n",
       " '第',\n",
       " '叮咚',\n",
       " '对',\n",
       " '对于',\n",
       " '多',\n",
       " '多少',\n",
       " '而',\n",
       " '而况',\n",
       " '而且',\n",
       " '而是',\n",
       " '而外',\n",
       " '而言',\n",
       " '而已',\n",
       " '尔后',\n",
       " '反过来',\n",
       " '反过来说',\n",
       " '反之',\n",
       " '非但',\n",
       " '非徒',\n",
       " '否则',\n",
       " '嘎',\n",
       " '嘎登',\n",
       " '该',\n",
       " '赶',\n",
       " '个',\n",
       " '各',\n",
       " '各个',\n",
       " '各位',\n",
       " '各种',\n",
       " '各自',\n",
       " '给',\n",
       " '根据',\n",
       " '跟',\n",
       " '故',\n",
       " '故此',\n",
       " '固然',\n",
       " '关于',\n",
       " '管',\n",
       " '归',\n",
       " '果然',\n",
       " '果真',\n",
       " '过',\n",
       " '哈',\n",
       " '哈哈',\n",
       " '呵',\n",
       " '和',\n",
       " '何',\n",
       " '何处',\n",
       " '何况',\n",
       " '何时',\n",
       " '嘿',\n",
       " '哼',\n",
       " '哼唷',\n",
       " '呼哧',\n",
       " '乎',\n",
       " '哗',\n",
       " '还是',\n",
       " '还有',\n",
       " '换句话说',\n",
       " '换言之',\n",
       " '或',\n",
       " '或是',\n",
       " '或者',\n",
       " '极了',\n",
       " '及',\n",
       " '及其',\n",
       " '及至',\n",
       " '即',\n",
       " '即便',\n",
       " '即或',\n",
       " '即令',\n",
       " '即若',\n",
       " '即使',\n",
       " '几',\n",
       " '几时',\n",
       " '己',\n",
       " '既',\n",
       " '既然',\n",
       " '既是',\n",
       " '继而',\n",
       " '加之',\n",
       " '假如',\n",
       " '假若',\n",
       " '假使',\n",
       " '鉴于',\n",
       " '将',\n",
       " '较',\n",
       " '较之',\n",
       " '叫',\n",
       " '接着',\n",
       " '结果',\n",
       " '借',\n",
       " '紧接着',\n",
       " '进而',\n",
       " '尽',\n",
       " '尽管',\n",
       " '经',\n",
       " '经过',\n",
       " '就',\n",
       " '就是',\n",
       " '就是说',\n",
       " '据',\n",
       " '具体地说',\n",
       " '具体说来',\n",
       " '开始',\n",
       " '开外',\n",
       " '靠',\n",
       " '咳',\n",
       " '可',\n",
       " '可见',\n",
       " '可是',\n",
       " '可以',\n",
       " '况且',\n",
       " '啦',\n",
       " '来',\n",
       " '来着',\n",
       " '离',\n",
       " '例如',\n",
       " '哩',\n",
       " '连',\n",
       " '连同',\n",
       " '两者',\n",
       " '了',\n",
       " '临',\n",
       " '另',\n",
       " '另外',\n",
       " '另一方面',\n",
       " '论',\n",
       " '嘛',\n",
       " '吗',\n",
       " '慢说',\n",
       " '漫说',\n",
       " '冒',\n",
       " '么',\n",
       " '每',\n",
       " '每当',\n",
       " '们',\n",
       " '莫若',\n",
       " '某',\n",
       " '某个',\n",
       " '某些',\n",
       " '拿',\n",
       " '哪',\n",
       " '哪边',\n",
       " '哪儿',\n",
       " '哪个',\n",
       " '哪里',\n",
       " '哪年',\n",
       " '哪怕',\n",
       " '哪天',\n",
       " '哪些',\n",
       " '哪样',\n",
       " '那',\n",
       " '那边',\n",
       " '那儿',\n",
       " '那个',\n",
       " '那会儿',\n",
       " '那里',\n",
       " '那么',\n",
       " '那么些',\n",
       " '那么样',\n",
       " '那时',\n",
       " '那些',\n",
       " '那样',\n",
       " '乃',\n",
       " '乃至',\n",
       " '呢',\n",
       " '能',\n",
       " '你',\n",
       " '你们',\n",
       " '您',\n",
       " '宁',\n",
       " '宁可',\n",
       " '宁肯',\n",
       " '宁愿',\n",
       " '哦',\n",
       " '呕',\n",
       " '啪达',\n",
       " '旁人',\n",
       " '呸',\n",
       " '凭',\n",
       " '凭借',\n",
       " '其',\n",
       " '其次',\n",
       " '其二',\n",
       " '其他',\n",
       " '其它',\n",
       " '其一',\n",
       " '其余',\n",
       " '其中',\n",
       " '起',\n",
       " '起见',\n",
       " '起见',\n",
       " '岂但',\n",
       " '恰恰相反',\n",
       " '前后',\n",
       " '前者',\n",
       " '且',\n",
       " '然而',\n",
       " '然后',\n",
       " '然则',\n",
       " '让',\n",
       " '人家',\n",
       " '任',\n",
       " '任何',\n",
       " '任凭',\n",
       " '如',\n",
       " '如此',\n",
       " '如果',\n",
       " '如何',\n",
       " '如其',\n",
       " '如若',\n",
       " '如上所述',\n",
       " '若',\n",
       " '若非',\n",
       " '若是',\n",
       " '啥',\n",
       " '上下',\n",
       " '尚且',\n",
       " '设若',\n",
       " '设使',\n",
       " '甚而',\n",
       " '甚么',\n",
       " '甚至',\n",
       " '省得',\n",
       " '时候',\n",
       " '什么',\n",
       " '什么样',\n",
       " '使得',\n",
       " '是',\n",
       " '是的',\n",
       " '首先',\n",
       " '谁',\n",
       " '谁知',\n",
       " '顺',\n",
       " '顺着',\n",
       " '似的',\n",
       " '虽',\n",
       " '虽然',\n",
       " '虽说',\n",
       " '虽则',\n",
       " '随',\n",
       " '随着',\n",
       " '所',\n",
       " '所以',\n",
       " '他',\n",
       " '他们',\n",
       " '他人',\n",
       " '它',\n",
       " '它们',\n",
       " '她',\n",
       " '她们',\n",
       " '倘',\n",
       " '倘或',\n",
       " '倘然',\n",
       " '倘若',\n",
       " '倘使',\n",
       " '腾',\n",
       " '替',\n",
       " '通过',\n",
       " '同',\n",
       " '同时',\n",
       " '哇',\n",
       " '万一',\n",
       " '往',\n",
       " '望',\n",
       " '为',\n",
       " '为何',\n",
       " '为了',\n",
       " '为什么',\n",
       " '为着',\n",
       " '喂',\n",
       " '嗡嗡',\n",
       " '我',\n",
       " '我们',\n",
       " '呜',\n",
       " '呜呼',\n",
       " '乌乎',\n",
       " '无论',\n",
       " '无宁',\n",
       " '毋宁',\n",
       " '嘻',\n",
       " '吓',\n",
       " '相对而言',\n",
       " '像',\n",
       " '向',\n",
       " '向着',\n",
       " '嘘',\n",
       " '呀',\n",
       " '焉',\n",
       " '沿',\n",
       " '沿着',\n",
       " '要',\n",
       " '要不',\n",
       " '要不然',\n",
       " '要不是',\n",
       " '要么',\n",
       " '要是',\n",
       " '也',\n",
       " '也罢',\n",
       " '也好',\n",
       " '一',\n",
       " '一般',\n",
       " '一旦',\n",
       " '一方面',\n",
       " '一来',\n",
       " '一切',\n",
       " '一样',\n",
       " '一则',\n",
       " '依',\n",
       " '依照',\n",
       " '矣',\n",
       " '以',\n",
       " '以便',\n",
       " '以及',\n",
       " '以免',\n",
       " '以至',\n",
       " '以至于',\n",
       " '以致',\n",
       " '抑或',\n",
       " '因',\n",
       " '因此',\n",
       " '因而',\n",
       " '因为',\n",
       " '哟',\n",
       " '用',\n",
       " '由',\n",
       " '由此可见',\n",
       " '由于',\n",
       " '有',\n",
       " '有的',\n",
       " '有关',\n",
       " '有些',\n",
       " '又',\n",
       " '于',\n",
       " '于是',\n",
       " '于是乎',\n",
       " '与',\n",
       " '与此同时',\n",
       " '与否',\n",
       " '与其',\n",
       " '越是',\n",
       " '云云',\n",
       " '哉',\n",
       " '再说',\n",
       " '再者',\n",
       " '在',\n",
       " '在下',\n",
       " '咱',\n",
       " '咱们',\n",
       " '则',\n",
       " '怎',\n",
       " '怎么',\n",
       " '怎么办',\n",
       " '怎么样',\n",
       " '怎样',\n",
       " '咋',\n",
       " '照',\n",
       " '照着',\n",
       " '者',\n",
       " '这',\n",
       " '这边',\n",
       " '这儿',\n",
       " '这个',\n",
       " '这会儿',\n",
       " '这就是说',\n",
       " '这里',\n",
       " '这么',\n",
       " '这么点儿',\n",
       " '这么些',\n",
       " '这么样',\n",
       " '这时',\n",
       " '这些',\n",
       " '这样',\n",
       " '正如',\n",
       " '吱',\n",
       " '之',\n",
       " '之类',\n",
       " '之所以',\n",
       " '之一',\n",
       " '只是',\n",
       " '只限',\n",
       " '只要',\n",
       " '只有',\n",
       " '至',\n",
       " '至于',\n",
       " '诸位',\n",
       " '着',\n",
       " '着呢',\n",
       " '自',\n",
       " '自从',\n",
       " '自个儿',\n",
       " '自各儿',\n",
       " '自己',\n",
       " '自家',\n",
       " '自身',\n",
       " '综上所述',\n",
       " '总的来看',\n",
       " '总的来说',\n",
       " '总的说来',\n",
       " '总而言之',\n",
       " '总之',\n",
       " '纵',\n",
       " '纵令',\n",
       " '纵然',\n",
       " '纵使',\n",
       " '遵照',\n",
       " '作为',\n",
       " '兮',\n",
       " '呃',\n",
       " '呗',\n",
       " '咚',\n",
       " '咦',\n",
       " '喏',\n",
       " '啐',\n",
       " '喔唷',\n",
       " '嗬',\n",
       " '嗯',\n",
       " '嗳',\n",
       " '']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['exp', 'lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', '１２', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "c:\\Users\\a\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:391: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['exp', 'lex', '①①', '①②', '①③', '①④', '①⑤', '①⑥', '①⑦', '①⑧', '①⑨', '①ａ', '①ｂ', '①ｃ', '①ｄ', '①ｅ', '①ｆ', '①ｇ', '①ｈ', '①ｉ', '①ｏ', '②①', '②②', '②③', '②④', '②⑤', '②⑥', '②⑦', '②⑧', '②⑩', '②ａ', '②ｂ', '②ｄ', '②ｅ', '②ｆ', '②ｇ', '②ｈ', '②ｉ', '②ｊ', '③①', '③⑩', '③ａ', '③ｂ', '③ｃ', '③ｄ', '③ｅ', '③ｆ', '③ｇ', '③ｈ', '④ａ', '④ｂ', '④ｃ', '④ｄ', '④ｅ', '⑤ａ', '⑤ｂ', '⑤ｄ', '⑤ｅ', '⑤ｆ', 'ｌｉ', 'ｚｘｆｉｔｌ'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "使用TfidfVectorizer()和 CountVectorizer()分别对数据进行特征的提取，投放到不同的模型中进行实验\n",
    "'''\n",
    "#开始使用TF-IDF进行特征的提取，对分词后的中文语句做向量化。\n",
    "#引进TF-IDF的包\n",
    "TF_Vec=TfidfVectorizer(max_df=0.8,\n",
    "                       min_df = 3,\n",
    "                       stop_words=frozenset(stopwords)\n",
    "                      )\n",
    "#拟合数据，将数据准转为标准形式，一般使用在训练集中\n",
    "train_x_tfvec=TF_Vec.fit_transform(train_x)\n",
    "#通过中心化和缩放实现标准化，一般使用在测试集中\n",
    "test_x_tfvec=TF_Vec.transform(test_x)\n",
    " \n",
    "#开始使用CountVectorizer()进行特征的提取。它依据词语出现频率转化向量。并且加入了去除停用词\n",
    "CT_Vec=CountVectorizer(max_df=0.8,#在超过这一比例的文档中出现的关键词（过于平凡），去除掉。\n",
    "                       min_df = 3,#在低于这一数量的文档中出现的关键词（过于独特），去除掉。\n",
    "                       token_pattern=u'(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b',#使用正则表达式，去除想去除的内容\n",
    "                       stop_words=frozenset(stopwords))#加入停用词)\n",
    "#拟合数据，将数据转化为标准形式，一般使用在训练集中\n",
    "train_x_ctvec=CT_Vec.fit_transform(train_x)\n",
    "#通过中心化和缩放实现标准化，一般使用在测试集中\n",
    "test_x_ctvec=CT_Vec.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最优参数： {'C': 1.8873918221350972, 'penalty': 'l2'}\n",
      "使用TF-IDF提取特征使用逻辑回归,让模型自适应参数，进行模型优化\n",
      "训练集:0.9967643051771117\n",
      "测试集:0.6995912806539509\n",
      "Precision_score 0.6410988473480499\n",
      "Recall_score 0.5153710968208768\n",
      "F1_score 0.4940038081977776\n",
      "\n",
      "使用模型优化的程序运行时间为 162.13877081871033\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "使用TF_IDF提取的向量当作数据特征传入模型\n",
    "'''\n",
    "#构建模型之前首先将包进行导入\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import time\n",
    "start_time=time.time()\n",
    "#创建模型\n",
    "lr = linear_model.LogisticRegression(penalty='l2', C=1, solver='liblinear', max_iter=1000, multi_class='ovr')\n",
    "#进行模型的优化，因为一些参数是不确定的，所以就让模型自己在训练中去确定自己的参数 模型的名字也由LR转变为model\n",
    "model = GridSearchCV(lr, cv=3, param_grid={\n",
    "        'C': np.logspace(0, 4, 30),\n",
    "        'penalty': ['l1', 'l2']\n",
    "    })\n",
    "#模型拟合tf-idf拿到的数据\n",
    "model.fit(train_x_tfvec,train_y)\n",
    "#查看模型自己拟合的最优参数\n",
    "print('最优参数：', model.best_params_)\n",
    "#在训练集上的正确率\n",
    "train_accracy=accuracy_score(train_y,pre_train_y)\n",
    "#训练结束查看预测 输入测试集查看预测\n",
    "pre_test_y=model.predict(test_x_tfvec)\n",
    "#查看在测试集上的准确率\n",
    "test_accracy = accuracy_score(test_y,pre_test_y)\n",
    "F1_score=f1_score(test_y,pre_test_y,average='macro')\n",
    "Recall_score=recall_score(test_y,pre_test_y,average='macro')\n",
    "Precision_score=precision_score(test_y,pre_test_y,average='macro')\n",
    "print('使用TF-IDF提取特征使用逻辑回归,让模型自适应参数，进行模型优化\\n训练集:{0}\\n测试集:{1}'.format(train_accracy,test_accracy))\n",
    "print(\"Precision_score\",Precision_score)\n",
    "print(\"Recall_score\",Recall_score)\n",
    "print(\"F1_score\",F1_score)\n",
    "end_time=time.time()\n",
    "print(\"\")\n",
    "print(\"使用模型优化的程序运行时间为\",end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用TfidfVectorizer提取特征使用KNN分类器的准确率\n",
      "训练集:0.5279291553133515\n",
      "测试集:0.4993188010899183\n",
      "Precision_score 0.43800697684192835\n",
      "Recall_score 0.3396952489510716\n",
      "F1_score 0.2395554676705456\n",
      "使用KNN分类器的程序运行时间为 2.509584903717041\n"
     ]
    }
   ],
   "source": [
    "#使用KNN模型\n",
    "import time\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "start_time=time.time()\n",
    "#创建模型\n",
    "Kn = KNeighborsClassifier()\n",
    "#拟合从tf-idf拿到的数据\n",
    "Kn.fit(train_x_tfvec,train_y)\n",
    "#在训练时查看训练集的准确率\n",
    "pre_train_y=Kn.predict(train_x_tfvec)\n",
    "#在训练集上的正确率\n",
    "train_accracy=accuracy_score(train_y,pre_train_y)\n",
    "#训练结束查看预测 输入测试集查看预测\n",
    "pre_test_y=Kn.predict(test_x_tfvec)\n",
    "#查看在测试集上的准确率\n",
    "test_accracy = accuracy_score(test_y,pre_test_y)\n",
    "F1_score=f1_score(test_y,pre_test_y,average='macro')\n",
    "Recall_score=recall_score(test_y,pre_test_y,average='macro')\n",
    "Precision_score=precision_score(test_y,pre_test_y,average='macro')\n",
    "print('使用TfidfVectorizer提取特征使用KNN分类器的准确率\\n训练集:{0}\\n测试集:{1}'.format(train_accracy,test_accracy))\n",
    "print(\"Precision_score\",Precision_score)\n",
    "print(\"Recall_score\",Recall_score)\n",
    "print(\"F1_score\",F1_score)\n",
    "end_time=time.time()\n",
    "print(\"使用KNN分类器的程序运行时间为\",end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用CounterfVectorizer提取特征使用随机森林分类器的准确率\n",
      "训练集:0.9737738419618529\n",
      "测试集:0.646457765667575\n",
      "Precision_score 0.5235176381727839\n",
      "Recall_score 0.48359791430956384\n",
      "F1_score 0.47500289504660415\n",
      "使用随机森林分类器的程序运行时间为 1.387753963470459\n"
     ]
    }
   ],
   "source": [
    "### Random Forest Classifier 随机森林分类器 \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import roc_curve, auc  ###计算roc和auc\n",
    "start_time=time.time()\n",
    "#创建模型\n",
    "Rfc = RandomForestClassifier(n_estimators=8)\n",
    "#拟合从CounterfVectorizer拿到的数据\n",
    "Rfc.fit(train_x_ctvec,train_y)\n",
    "#在训练时查看训练集的准确率\n",
    "pre_train_y=Rfc.predict(train_x_ctvec)\n",
    "#在训练集上的正确率\n",
    "train_accracy=accuracy_score(train_y,pre_train_y)\n",
    "#训练结束查看预测 输入测试集查看预测\n",
    "pre_test_y=Rfc.predict(test_x_ctvec)\n",
    "#查看在测试集上的准确率\n",
    "test_accracy = accuracy_score(test_y,pre_test_y)\n",
    "F1_score=f1_score(test_y,pre_test_y,average='macro')\n",
    "Recall_score=recall_score(test_y,pre_test_y,average='macro')\n",
    "Precision_score=precision_score(test_y,pre_test_y,average='macro')\n",
    "print('使用CounterfVectorizer提取特征使用随机森林分类器的准确率\\n训练集:{0}\\n测试集:{1}'.format(train_accracy,test_accracy))\n",
    "print(\"Precision_score\",Precision_score)\n",
    "print(\"Recall_score\",Recall_score)\n",
    "print(\"F1_score\",F1_score)\n",
    "end_time=time.time()\n",
    "print(\"使用随机森林分类器的程序运行时间为\",end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用tf提取特征使用决策树分类器的准确率\n",
      "训练集:0.9967643051771117\n",
      "测试集:0.6362397820163488\n",
      "Precision_score 0.5232498515679819\n",
      "Recall_score 0.47662469387797274\n",
      "F1_score 0.47009327498902276\n",
      "使用决策树分类器的程序运行时间为 8.262097358703613\n"
     ]
    }
   ],
   "source": [
    "### Decision Tree Classifier  决策树\n",
    "from sklearn import tree\n",
    "import time\n",
    "start_time=time.time()\n",
    "#创建模型\n",
    "Rf = tree.DecisionTreeClassifier()\n",
    "#拟合从tf-idf拿到的数据\n",
    "Rf.fit(train_x_tfvec,train_y)\n",
    "#在训练时查看训练集的准确率\n",
    "pre_train_y=Rf.predict(train_x_tfvec)\n",
    "#在训练集上的正确率\n",
    "train_accracy=accuracy_score(train_y,pre_train_y)\n",
    "#训练结束查看预测 输入测试集查看预测\n",
    "pre_test_y=Rfc.predict(test_x_ctvec)\n",
    "#查看在测试集上的准确率\n",
    "test_accracy = accuracy_score(test_y,pre_test_y)\n",
    "F1_score=f1_score(test_y,pre_test_y,average='macro')\n",
    "Recall_score=recall_score(test_y,pre_test_y,average='macro')\n",
    "Precision_score=precision_score(test_y,pre_test_y,average='macro')\n",
    "print('使用tf提取特征使用决策树分类器的准确率\\n训练集:{0}\\n测试集:{1}'.format(train_accracy,test_accracy))\n",
    "print(\"Precision_score\",Precision_score)\n",
    "print(\"Recall_score\",Recall_score)\n",
    "print(\"F1_score\",F1_score)\n",
    "end_time=time.time()\n",
    "print(\"使用决策树分类器的程序运行时间为\",end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用CounterVectorizer提取特征使用贝叶斯分类器的准确率\n",
      "训练集:0.9967643051771117\n",
      "测试集:0.6185286103542235\n",
      "Precision_score 0.5399800928306252\n",
      "Recall_score 0.5584755922961474\n",
      "F1_score 0.5401647116167149\n",
      "使用贝叶斯分类器的程序运行时间为 0.01701211929321289\n"
     ]
    }
   ],
   "source": [
    "### 贝叶斯\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import time\n",
    "start_time=time.time()\n",
    "#创建模型\n",
    "Bys = MultinomialNB()\n",
    "#拟合数据\n",
    "Bys.fit(train_x_ctvec, train_y)# 学习,拟合模型\n",
    "#在训练集上的正确率\n",
    "train_accracy=accuracy_score(train_y,pre_train_y)\n",
    "#训练结束查看预测 输入测试集查看预测\n",
    "pre_test_y=Bys.predict(test_x_ctvec)\n",
    "#查看在测试集上的准确率\n",
    "test_accracy = accuracy_score(test_y,pre_test_y)\n",
    "F1_score=f1_score(test_y,pre_test_y,average='macro')\n",
    "Recall_score=recall_score(test_y,pre_test_y,average='macro')\n",
    "Precision_score=precision_score(test_y,pre_test_y,average='macro')\n",
    "print('使用CounterVectorizer提取特征使用贝叶斯分类器的准确率\\n训练集:{0}\\n测试集:{1}'.format(train_accracy,test_accracy))\n",
    "print(\"Precision_score\",Precision_score)\n",
    "print(\"Recall_score\",Recall_score)\n",
    "print(\"F1_score\",F1_score)\n",
    "end_time=time.time()\n",
    "print(\"使用贝叶斯分类器的程序运行时间为\",end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用CounterfVectorizer提取特征使用SVM分类器的准确率\n",
      "训练集:0.5279291553133515\n",
      "测试集:0.5088555858310627\n",
      "Precision_score 0.3929472766484651\n",
      "Recall_score 0.3484851644112679\n",
      "F1_score 0.26324953973988796\n",
      "使用SVM分类器的程序运行时间为 50.71697735786438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#使用SVM分类器\n",
    "from sklearn.svm import SVC\n",
    "import time\n",
    "start_time=time.time()\n",
    "#创建模型\n",
    "SVM = SVC(C=1.0, kernel='rbf', gamma='auto')\n",
    "#拟合数据\n",
    "SVM.fit(train_x_ctvec, train_y)# 学习,拟合模型\n",
    "#在训练集上的正确率\n",
    "train_accracy=accuracy_score(train_y,pre_train_y)\n",
    "#训练结束查看预测 输入测试集查看预测\n",
    "pre_test_y=SVM.predict(test_x_ctvec)\n",
    "#查看在测试集上的准确率\n",
    "test_accracy = accuracy_score(test_y,pre_test_y)\n",
    "F1_score=f1_score(test_y,pre_test_y,average='macro')\n",
    "Recall_score=recall_score(test_y,pre_test_y,average='macro')\n",
    "Precision_score=precision_score(test_y,pre_test_y,average='macro')\n",
    "print('使用CounterfVectorizer提取特征使用SVM分类器的准确率\\n训练集:{0}\\n测试集:{1}'.format(train_accracy,test_accracy))\n",
    "print(\"Precision_score\",Precision_score)\n",
    "print(\"Recall_score\",Recall_score)\n",
    "print(\"F1_score\",F1_score)\n",
    "end_time=time.time()\n",
    "print(\"使用SVM分类器的程序运行时间为\",end_time-start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1cac20a0be55dd1fe2f46a071e6ccfd0d69730ae8edcfbd71c03315cff00f8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
